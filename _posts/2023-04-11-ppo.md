---
title: "PPO(Proximal Policy Optimization) 알고리즘"
categories: RL
tags:
  - AI
  - ML
  - DL
  - RL
search: true
---
강화학습의 PPO(Proximal Policy Optimization) 개념을 공부하면서 정리한 내용을 올림

## 특징
1. 소비했던 데이터를 다시 쓰기(데이터 재사용)  
2. Episode가 끝난 뒤 결과를 반영하는데 아니라, step단위로 학습에 반영하기  

Reinforcement Learning에서의 목표함수 Expected Reward 함수 (최대화)  
<img width="247" alt="스크린샷 2023-04-11 오전 3 31 31" src="https://user-images.githubusercontent.com/86637300/231096165-9635a334-24ec-425d-b1e8-fea5c7303602.png">  


## PPO는 TRPO에서 파생된 알고리즘  
TRPO는 이전 스텝의 파라미터(θ_old)와 현재 스텝의 파라미터(θ)간의 차이를 이용하며  
이 차이가 매우 클 경우 제한을 주어 신뢰구간(Trust Region)내에서의 업데이트를 수행  
<img width="347" alt="스크린샷 2023-04-11 오전 3 50 13" src="https://user-images.githubusercontent.com/86637300/231096178-8d2a2f47-87fc-46a0-b63f-5aed2a83510f.png">  
* $$\pi_\theta$$ : stochastic policy  
* $$A_t$$ : 가치를 평가하는 네트워크에 의해 t의 시점에서 추정되는 Advantage  

하지만 TRPO에서는 constraint optimization 문제를 해결하는데 컴퓨터에게 많은 연산량을 요구  
PPO에서는 surrogate function을 업데이트할 때 신뢰 구간을 강제적으로 Clipping 기법으로 설정함으로써 연산량을 줄임  
<img width="583" alt="스크린샷 2023-04-11 오전 3 47 34" src="https://user-images.githubusercontent.com/86637300/231096187-1812a3b6-dc63-4026-96a6-20611a1b0c38.png">  
<img width="167" alt="스크린샷 2023-04-11 오전 3 54 27" src="https://user-images.githubusercontent.com/86637300/231096195-b230111c-335f-4804-a11e-f9e92d5e2efa.png">  
$$\epsilon$$ : 하이퍼파라미터 
* Continuous action일 때는 0.2 일 때가 가장 성능이 좋음
* Discrete action일 때는 0.1 × α 일 때가 가장 성능이 좋음 
* α는 학습률로서 1에서 시작하여 점점 0으로 수렴

<img width="861" alt="스크린샷 2023-04-11 오후 5 10 13" src="https://user-images.githubusercontent.com/86637300/231097707-9bfc251c-c42b-4b83-b14f-27a7296aa57b.png">  
Advantage가 양수라는 것은 가치가 현재보다 높다라는 것이며 파라미터를 양의 방향으로 업데이트  
상태 s에서 행동 a를 선택할 확률 증가하게 업데이트  
$$r_t$$(θ)가 아무리 커져도 ϵ으로 Clipping 함으로써 신뢰구간 내에서 파라미터 업데이트  

Advantage가 음수라는 것은 현재보다 가치가 낮다라는 것  
상태 s에서 행동 a를 선택할 확률을 떨어뜨리는 방향으로 Clipping 기법으로 신뢰구간 내에서 파라미터를 업데이트  


## Update Algorithm  
Advantage Actor Critic을 업데이트할 때와 같은 방법으로 업데이트 함  
PPO에서는 Surrogate Function($$L_CLIP$$)을 최대화하는 방향으로 그리고  
Action Entropy(S[$$π_θ$$])를 최대화 하는 방향으로 Actor 네트워크를 업데이트하며  
State-Value($$L^VF$$)의 차이를 최소화 하는 방향으로 Critic 네트워크를 업데이트 함  
<img width="592" alt="스크린샷 2023-04-11 오전 4 17 13" src="https://user-images.githubusercontent.com/86637300/231096236-974b8498-af8d-4a7a-b00e-543af0f06744.png">  


이를 하나씩 최적화 하는 기법을 기존의 Advantage Actor Critic에서 사용하고 있지만  
PPO에서는 하나의 식으로 통합하여 한번에 최적화를 수행  
<img width="556" alt="스크린샷 2023-04-11 오전 4 17 36" src="https://user-images.githubusercontent.com/86637300/231096251-e15a92a6-b5f4-4ad9-8be0-f5493d38c2f3.png">  
* $$c_1$$ : Value-State network 최적화 정도
* $$c_2$$ : Exploration의 정도


